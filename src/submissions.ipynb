{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "import requests\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from data import DataModule\n",
    "from models import Transformer\n",
    "from transformers import RobertaTokenizerFast\n",
    "import torch\n",
    "import numpy as np\n",
    "en_stopwords = stopwords.words('english')\n",
    "\n",
    "config = dotenv_values(\"../.env\")  # config = {\"USER\": \"foo\", \"EMAIL\": \"foo@example.org\"}\n",
    "debug_server = config['DEBUG_SERVER']\n",
    "token = config['TOKEN']\n",
    "submitt_url = config['DEBUG_SERVER_SUBMIT']\n",
    "HEADER = {'content-type': 'application/json'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_whitespace(text):\n",
    "   \"\"\" Removes all whitespaces from a given text.\"\"\"\n",
    "   return \" \".join(text.split())\n",
    "\n",
    "def remove_URLs(text):\n",
    "   \"Remove URLs from text using regular expressions.\"\n",
    "   url_re = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "   return url_re.sub(r'', text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "   tokenizer = RegexpTokenizer(r'\\w+')\n",
    "   no_punct = tokenizer.tokenize(\" \".join(text))\n",
    "   return no_punct\n",
    "\n",
    "def remove_stopwords(text):\n",
    "   \"\"\"Removes english stopwords.\"\"\"\n",
    "   result = []\n",
    "   for token in text:\n",
    "      if token not in en_stopwords:\n",
    "         result.append(token)\n",
    "\n",
    "   return result\n",
    "\n",
    "def stemming(text):\n",
    "   porter = PorterStemmer()\n",
    "   result = []\n",
    "   for word in text:\n",
    "      result.append(porter.stem(word))\n",
    "\n",
    "   return result\n",
    "\n",
    "def ml_text_processing(text):\n",
    "    text = text.lower()\n",
    "    text = remove_whitespace(text)\n",
    "    text = remove_URLs(text)\n",
    "    text = word_tokenize(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = stemming(text)\n",
    "    text = \" \".join(text)\n",
    "    return [text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPipeline():\n",
    "    def __init__(self,path_model, path_text_processor, proc_func):\n",
    "        self.text_preprocessing = proc_func\n",
    "        self.processor = pickle.load(open(path_text_processor, 'rb'))\n",
    "        self.model = pickle.load(open(path_model, 'rb'))\n",
    "    def predict(self, text):\n",
    "        return  int(self.model.predict(self.processor.transform(self.text_preprocessing(text))))\n",
    "    def process(self, text):\n",
    "        return self.processor.transform(self.text_preprocessing(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_path = r\"../models/ml_models/CV.sav\"\n",
    "tfidf_path = r\"../models/ml_models/TFIDF.sav\"\n",
    "model1 = MLPipeline(r\"../models/ml_models/m1_LR_CV.sav\", cv_path, ml_text_processing)\n",
    "model2 = MLPipeline(r\"../models/ml_models/m2_LR_TFIDF.sav\", tfidf_path, ml_text_processing)\n",
    "model3 = MLPipeline(r\"../models/ml_models/m3_RFC_CV.sav\", cv_path, ml_text_processing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = MLPipeline(f\"../models/ml_models/m4_RFC_TFIDF.sav\", tfidf_path, ml_text_processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = r\"../models/transformer_32/model-19-0.22.ckpt\"\n",
    "tokenizer_path =  r\"../models/roberta-tokenizer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DLModel():\n",
    "    def __init__(self, model_path, tokenizer_path):\n",
    "        self.model = Transformer(\n",
    "                        ntokens=30000,\n",
    "                        emsize=128,\n",
    "                        d_hid=128,\n",
    "                        nlayers=1,\n",
    "                        nhead=2,\n",
    "                        dropout=0.2,).load_from_checkpoint(model_path)\n",
    "        self.model.eval()\n",
    "        self.tokenizer = RobertaTokenizerFast.from_pretrained(tokenizer_path)\n",
    "    def predict(self,text):\n",
    "        text = self.tokenizer.encode_plus(\n",
    "                text,\n",
    "                # add_special_tokens=True,\n",
    "                max_length=512,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\",\n",
    "                return_attention_mask=False,\n",
    "                truncation=True,\n",
    "                )\n",
    "        output = torch.sigmoid(self.model(text['input_ids'])).detach().numpy()\n",
    "        return np.heaviside(output, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "file ../models/roberta-tokenizer/config.json not found\n",
      "file ../models/roberta-tokenizer/config.json not found\n"
     ]
    }
   ],
   "source": [
    "model0 = DLModel(model_path=model_path, tokenizer_path=tokenizer_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Official Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "```\n",
    "getWritings\n",
    "while request not empty:\n",
    "    for run in runs:\n",
    "        POST request\n",
    "    getWritings\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET_URL = 'http://127.0.0.1:5000'\n",
    "GET_URL = config['T1_SERVER'] + token\n",
    "systems = [model0, model1, model2, model3, model4]\n",
    "ans = requests.get(GET_URL)\n",
    "ans_dict = json.loads(ans.text)\n",
    "while ans_dict != []:\n",
    "    for run, system in enumerate(systems):\n",
    "        post_url = config['T1_SUBMISSION'] + token + \"/\" + str(run)\n",
    "        submission = []\n",
    "        for ans in ans_dict:\n",
    "            nick = ans['nick']\n",
    "            text = ans['content']\n",
    "            submission.append({\n",
    "                'nick': nick,\n",
    "                'decision': system.predict(text),\n",
    "                'score': round(random.uniform(0,4),1)\n",
    "            })\n",
    "        post_request = requests.post(post_url, data = json.dumps(submission), headers = HEADER)\n",
    "    ans = requests.get(GET_URL)\n",
    "    ans_dict = json.loads(ans.text)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://erisk.irlab.org/challenge-t1/getwritings/gvq8q2KMn564w+0RADA15Hv3DfqPYXb2c6czyVxI5U8\n"
     ]
    }
   ],
   "source": [
    "url = f\"https://erisk.irlab.org/challenge-t1/getwritings/{token}\"\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[]'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bc7067ad0587ee2533d0568fff1e23349b43564da49d2e99835e9871e4935780"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
