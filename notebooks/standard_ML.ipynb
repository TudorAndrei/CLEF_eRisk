{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the subjects' label.\n",
    "path_labels = '../data/risk_golden_truth.txt'\n",
    "labels = {}\n",
    "with open(path_labels, 'r') as f:\n",
    "    for line in f:\n",
    "        subject, label = line.split()\n",
    "        labels[subject] = int(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the train dataset from the subjects' posts and label.\n",
    "txt_path = glob('../data/processed/*')\n",
    "rows_data = []\n",
    "for path in txt_path:\n",
    "   with open(path, 'r+') as f:\n",
    "      # Split the path, get the filename \n",
    "      # (which contains the subject) and remove '.txt'\n",
    "      subject = path.split('\\\\')[1][:-4] \n",
    "      txt = f.read()\n",
    "      rows_data.append((subject, txt, labels[subject]))\n",
    "\n",
    "# Training dataset.\n",
    "train_data = pd.DataFrame(rows_data, columns=['Subject', 'Text', 'Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subject1</td>\n",
       "      <td>Vulcan's ultimate landing at max range is so s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subject100</td>\n",
       "      <td>I almost only play Marathon/Huge. I enjoy slow...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subject1006</td>\n",
       "      <td>http://imgur.com/igov5qk http://imgur.com/vHu8...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>subject1008</td>\n",
       "      <td>What are the list of softwares you use for day...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>subject1011</td>\n",
       "      <td>Subscribe and like will help me #128522;\\nLoL\\...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2343</th>\n",
       "      <td>subject9980</td>\n",
       "      <td>\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2344</th>\n",
       "      <td>subject9981</td>\n",
       "      <td>\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345</th>\n",
       "      <td>subject9986</td>\n",
       "      <td>Checkout r/GoGoJoJo against the qualified immu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2346</th>\n",
       "      <td>subject9990</td>\n",
       "      <td>Only took about 10 tries to post correctly lol...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2347</th>\n",
       "      <td>subject9996</td>\n",
       "      <td>I see you need 1 more kitsune, so what you wan...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2348 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Subject                                               Text  Label\n",
       "0        subject1  Vulcan's ultimate landing at max range is so s...      0\n",
       "1      subject100  I almost only play Marathon/Huge. I enjoy slow...      0\n",
       "2     subject1006  http://imgur.com/igov5qk http://imgur.com/vHu8...      0\n",
       "3     subject1008  What are the list of softwares you use for day...      0\n",
       "4     subject1011  Subscribe and like will help me #128522;\\nLoL\\...      0\n",
       "...           ...                                                ...    ...\n",
       "2343  subject9980   \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n ...      0\n",
       "2344  subject9981   \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n ...      0\n",
       "2345  subject9986  Checkout r/GoGoJoJo against the qualified immu...      0\n",
       "2346  subject9990  Only took about 10 tries to post correctly lol...      0\n",
       "2347  subject9996  I see you need 1 more kitsune, so what you wan...      0\n",
       "\n",
       "[2348 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the subjects' posts\n",
    "Text normalization techniques:\n",
    "- lowercasing \n",
    "- whitespace removal\n",
    "- URL removal\n",
    "- tokenization\n",
    "- stopwords removal\n",
    "- punctuation removal\n",
    "- stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize all words to lowercase.\n",
    "train_data[\"Text\"] = train_data[\"Text\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_whitespace(text):\n",
    "   \"\"\" Removes all whitespaces from a given text.\"\"\"\n",
    "   return \" \".join(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all whitespaces from texts.\n",
    "train_data[\"Text\"] = train_data[\"Text\"].apply(remove_whitespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_URLs(text):\n",
    "   \"Remove URLs from text using regular expressions.\"\n",
    "   url_re = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "   return url_re.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://imgur.com/igov5qk http://imgur.com/vhu8cjn this was just a lucky run all around. i got some good crew early and then got good system levels like shields and engines. this let me focus on a weap'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"Text\"][2][:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove URLs from texts.\n",
    "train_data[\"Text\"] = train_data[\"Text\"].apply(remove_URLs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  this was just a lucky run all around. i got some good crew early and then got good system levels like shields and engines. this let me focus on a weapons system, and someone truly blessed me. triple'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"Text\"][2][:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract tokens from texts.\n",
    "train_data[\"Text\"] = train_data[\"Text\"].apply(lambda x: word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "   tokenizer = RegexpTokenizer(r'\\w+')\n",
    "   no_punct = tokenizer.tokenize(\" \".join(text))\n",
    "   return no_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes punctuation from texts.\n",
    "train_data[\"Text\"] = train_data[\"Text\"].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stopwords = stopwords.words('english')\n",
    "def remove_stopwords(text):\n",
    "   \"\"\"Removes english stopwords.\"\"\"\n",
    "   result = []\n",
    "   for token in text:\n",
    "      if token not in en_stopwords:\n",
    "         result.append(token)\n",
    "\n",
    "   return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes stopwords from texts.\n",
    "train_data[\"Text\"] = train_data[\"Text\"].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(text):\n",
    "   porter = PorterStemmer()\n",
    "   result = []\n",
    "   for word in text:\n",
    "      result.append(porter.stem(word))\n",
    "\n",
    "   return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply stemming to words from texts.\n",
    "train_data[\"Text\"] = train_data[\"Text\"].apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset to a .csv file.\n",
    "train_data.to_csv('../data/train_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard ML approaches:\n",
    "- LinearSVC\n",
    "- Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training dataset.\n",
    "train_data = pd.read_csv('../data/train_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the distribution of data labels.\n",
    "train_data[\"Label\"].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model - Linear SVC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data[\"Text\"].apply(\" \".join)\n",
    "y = train_data[\"Label\"]\n",
    "# Extract BOW using CountVectorizer().\n",
    "feature_extractor = CountVectorizer(lowercase=False)\n",
    "classifier = LinearSVC(max_iter=1000, class_weight='balanced')\n",
    "\n",
    "pipeline = Pipeline([('feature extractor', feature_extractor), ('classifier', classifier)])\n",
    "# Use a stratified 5-fold CV \n",
    "# to try to reduce the class imbalance in train/test splits.\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "scores = cross_val_score(pipeline, X, y, cv=cv, scoring='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for BOW + LinearSVC: 0.8664433067750116\n"
     ]
    }
   ],
   "source": [
    "print(\"F1 score for BOW + LinearSVC:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML experiments:\n",
    "- Logistic Regression\n",
    "- Random Forest\n",
    "- K-neighbors\n",
    "- Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  1.03  min\n",
      "Experiment:  CountVectorizer() + LinearSVC()\n",
      "F1 score= [0.82191781 0.87878788 0.84848485 0.85294118 0.8358209 ]  mean= 0.8475905214969763\n",
      "====================================================================================================\n",
      "Time:  2.65  min\n",
      "Experiment:  CountVectorizer() + LogisticRegression(max_iter=1000)\n",
      "F1 score= [0.84057971 0.90625    0.84848485 0.87878788 0.8358209 ]  mean= 0.8619846665880087\n",
      "====================================================================================================\n",
      "Time:  1.19  min\n",
      "Experiment:  CountVectorizer() + RandomForestClassifier()\n",
      "F1 score= [0.35       0.42857143 0.42857143 0.51162791 0.46511628]  mean= 0.4367774086378738\n",
      "====================================================================================================\n",
      "Time:  1.09  min\n",
      "Experiment:  CountVectorizer() + KNeighborsClassifier()\n",
      "F1 score= [0.75       0.73076923 0.74074074 0.79245283 0.77192982]  mean= 0.7571785252520108\n",
      "====================================================================================================\n",
      "Time:  1.23  min\n",
      "Experiment:  CountVectorizer() + DecisionTreeClassifier()\n",
      "F1 score= [0.90909091 0.87096774 0.86666667 0.92063492 0.86153846]  mean= 0.8857797399732883\n",
      "====================================================================================================\n",
      "Time:  1.04  min\n",
      "Experiment:  TfidfVectorizer() + LinearSVC()\n",
      "F1 score= [0.88135593 0.88135593 0.86206897 0.95081967 0.88135593]  mean= 0.8913912868517118\n",
      "====================================================================================================\n",
      "Time:  1.21  min\n",
      "Experiment:  TfidfVectorizer() + LogisticRegression(max_iter=1000)\n",
      "F1 score= [0.77777778 0.73076923 0.77777778 0.87719298 0.82142857]  mean= 0.7969892680418996\n",
      "====================================================================================================\n",
      "Time:  1.23  min\n",
      "Experiment:  TfidfVectorizer() + RandomForestClassifier()\n",
      "F1 score= [0.56521739 0.59574468 0.42857143 0.57777778 0.59574468]  mean= 0.5526111918711363\n",
      "====================================================================================================\n",
      "Time:  1.1  min\n",
      "Experiment:  TfidfVectorizer() + KNeighborsClassifier()\n",
      "F1 score= [0.77777778 0.75       0.71698113 0.86666667 0.77192982]  mean= 0.7766710802162639\n",
      "====================================================================================================\n",
      "Time:  1.23  min\n",
      "Experiment:  TfidfVectorizer() + DecisionTreeClassifier()\n",
      "F1 score= [0.89552239 0.90909091 0.90625    0.86153846 0.89552239]  mean= 0.8935848293497546\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "feature_extractors = [CountVectorizer(), TfidfVectorizer()]\n",
    "classifiers = [LinearSVC(max_iter=1000), LogisticRegression(max_iter=1000), RandomForestClassifier(), KNeighborsClassifier(), DecisionTreeClassifier()]\n",
    "\n",
    "for extractor in feature_extractors:\n",
    "   for classifier in classifiers:\n",
    "      start_time = time.time()\n",
    "      pipeline = Pipeline([('feature extractor', extractor), ('classifier', classifier)])\n",
    "      cv = StratifiedKFold(n_splits=5)\n",
    "      scores = cross_val_score(pipeline, X, y, cv = cv, scoring='f1')\n",
    "      end_time = time.time()\n",
    "      \n",
    "      print(\"Time: \", round((end_time - start_time)/60, 2), \" min\")\n",
    "      print(\"Experiment: \", str(extractor), \"+\", str(classifier))\n",
    "      print(\"F1 score=\", scores, \" mean=\", np.mean(scores))\n",
    "      print(\"=\"*100)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c2df39527199e9c3424817245acb9a622fe910bfbc420b32efee2c316061d83c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('bioNLP_venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
