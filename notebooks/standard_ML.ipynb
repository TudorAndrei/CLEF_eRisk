{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the subjects' label.\n",
    "path_labels = '../data/risk_golden_truth.txt'\n",
    "labels = {}\n",
    "with open(path_labels, 'r') as f:\n",
    "    for line in f:\n",
    "        subject, label = line.split()\n",
    "        labels[subject] = int(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset():\n",
    "   # Create the train dataset from the subjects' posts and label.\n",
    "   txt_path = glob('../data/processed/*')\n",
    "   rows_data = []\n",
    "   for path in txt_path:\n",
    "      with open(path, 'r+') as f:\n",
    "         # Split the path, get the filename \n",
    "         # (which contains the subject) and remove '.txt'\n",
    "         subject = path.split('\\\\')[1][:-4] \n",
    "         txt = f.read()\n",
    "         rows_data.append((subject, txt, labels[subject]))\n",
    "\n",
    "   # Training dataset.\n",
    "   train_data = pd.DataFrame(rows_data, columns=['Subject', 'Text', 'Label'])\n",
    "   return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = create_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the subjects' posts\n",
    "Text normalization techniques:\n",
    "- lowercasing \n",
    "- whitespace removal\n",
    "- URL removal\n",
    "- tokenization\n",
    "- stopwords removal\n",
    "- punctuation removal\n",
    "- stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize all words to lowercase.\n",
    "train_data[\"Text\"] = train_data[\"Text\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_whitespace(text):\n",
    "   \"\"\" Removes all whitespaces from a given text.\"\"\"\n",
    "   return \" \".join(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all whitespaces from texts.\n",
    "train_data[\"Text\"] = train_data[\"Text\"].apply(remove_whitespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_URLs(text):\n",
    "   \"Remove URLs from text using regular expressions.\"\n",
    "   url_re = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "   return url_re.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"Text\"][2][:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove URLs from texts.\n",
    "train_data[\"Text\"] = train_data[\"Text\"].apply(remove_URLs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"Text\"][2][:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract tokens from texts.\n",
    "train_data[\"Text\"] = train_data[\"Text\"].apply(lambda x: word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "   tokenizer = RegexpTokenizer(r'\\w+')\n",
    "   no_punct = tokenizer.tokenize(\" \".join(text))\n",
    "   return no_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes punctuation from texts.\n",
    "train_data[\"Text\"] = train_data[\"Text\"].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stopwords = stopwords.words('english')\n",
    "def remove_stopwords(text):\n",
    "   \"\"\"Removes english stopwords.\"\"\"\n",
    "   result = []\n",
    "   for token in text:\n",
    "      if token not in en_stopwords:\n",
    "         result.append(token)\n",
    "\n",
    "   return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes stopwords from texts.\n",
    "train_data[\"Text\"] = train_data[\"Text\"].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(text):\n",
    "   porter = PorterStemmer()\n",
    "   result = []\n",
    "   for word in text:\n",
    "      result.append(porter.stem(word))\n",
    "\n",
    "   return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply stemming to words from texts.\n",
    "train_data[\"Text\"] = train_data[\"Text\"].apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset to a .csv file.\n",
    "train_data.to_csv('../data/train_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard ML approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training dataset.\n",
    "train_data = pd.read_csv('../data/train_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of class labels.\n",
    "train_data[\"Label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of class labels.\n",
    "train_data[\"Label\"].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML experiments\n",
    "Feature extractors:\n",
    "- Bag-of-Words\n",
    "- TF-IDF\n",
    "\n",
    "Classifiers:\n",
    "- LinearSVC\n",
    "- Logistic Regression\n",
    "- Random Forest\n",
    "- K-neighbors\n",
    "- Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data[\"Text\"]\n",
    "y = train_data[\"Label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extractors.\n",
    "feature_extractors = [\n",
    "CountVectorizer(),\n",
    "TfidfVectorizer(),\n",
    "# Ignore words that appear in less than 20% of posts (rare words).\n",
    "CountVectorizer(min_df=0.2),\n",
    "# Ignore words that appear in more than 80% of posts (frequent words).\n",
    "CountVectorizer(max_df=0.8),\n",
    "CountVectorizer(ngram_range=(2, 2)),\n",
    "CountVectorizer(ngram_range=(3, 3)),\n",
    "TfidfVectorizer(min_df=0.2),\n",
    "TfidfVectorizer(max_df=0.8),\n",
    "TfidfVectorizer(ngram_range=(2, 2)),\n",
    "TfidfVectorizer(ngram_range=(3, 3))\n",
    "]\n",
    "\n",
    "# Classifiers.\n",
    "classifiers = [ \n",
    "# LinearSVC(max_iter=100000), \n",
    "# LogisticRegression(max_iter=100000),\n",
    "KNeighborsClassifier(),\n",
    "RandomForestClassifier(),\n",
    "DecisionTreeClassifier()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a stratified 5-fold CV to try to reduce\n",
    "# the class imbalance in train/validation splits.\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "scoring = {\n",
    "'accuracy': make_scorer(accuracy_score),\n",
    "'precision': make_scorer(precision_score),\n",
    "'recall': make_scorer(recall_score),\n",
    "'f1_score': make_scorer(f1_score)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_ML_pipeline():\n",
    "   for classifier in classifiers:\n",
    "      for extractor in feature_extractors:\n",
    "         start_time = time.time()\n",
    "         pipeline = Pipeline([('extractor', extractor), ('classifier', classifier)])\n",
    "         scores = cross_validate(pipeline, X, y, cv=cv, scoring=scoring)\n",
    "         end_time = time.time()\n",
    "         total_time = round((end_time - start_time)/60, 2)\n",
    "         \n",
    "         f1_score = round(np.mean(scores['test_f1_score']), 2)\n",
    "         precision = round(np.mean(scores['test_precision']), 2)\n",
    "         recall = round(np.mean(scores['test_recall']), 2)\n",
    "         acc = round(np.mean(scores['test_accuracy']), 2)\n",
    "\n",
    "         print(\"Time: \", total_time, \" min\")\n",
    "         print(\"Experiment: \", str(classifier), \"+\", str(extractor))\n",
    "         print(\"F1 score= \", f1_score, \", 5-fold CV=\", scores['test_f1_score'])\n",
    "         print(\"Precision= \", precision, \", 5-fold CV=\", scores['test_precision'])\n",
    "         print(\"Recall= \", recall, \", 5-fold CV=\", scores['test_recall'])\n",
    "         print(\"Accuracy= \", acc, \", 5-fold CV=\", scores['test_accuracy'])\n",
    "         print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_ML_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataset with individual posts:\n",
    "- labeled separately for label=1\n",
    "- chunks of 20 posts for label=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_single_posts():\n",
    "   # Create the train dataset from the subjects' posts and label.\n",
    "   txt_path = glob('../data/processed/*')\n",
    "   rows_data = []\n",
    "   chunks = 20\n",
    "   for path in txt_path:\n",
    "      with open(path, 'r+') as f:\n",
    "         # Split the path, get the filename \n",
    "         # (which contains the subject) and remove '.txt'\n",
    "         subject = path.split('\\\\')[1][:-4]\n",
    "         txt = f.read()\n",
    "         txt = txt.split('\\n')\n",
    "         label = labels[subject]\n",
    "         i = 0\n",
    "         chunked_posts = []\n",
    "         for post in txt:\n",
    "            # For the posts with label 1, store them individually.\n",
    "            if label == 1:\n",
    "               rows_data.append((subject, post, label))\n",
    "            # For the posts with label 0, form chunks of 20 posts.\n",
    "            else:\n",
    "               i +=1\n",
    "               chunked_posts.append(post)\n",
    "               if i == chunks:\n",
    "                  chunked_posts = \" \".join(chunked_posts)\n",
    "                  rows_data.append((subject, chunked_posts, label))\n",
    "                  chunked_posts = []\n",
    "                  i = 0\n",
    "\n",
    "   # Training dataset.\n",
    "   train_data = pd.DataFrame(rows_data, columns=['Subject', 'Text', 'Label'])\n",
    "   return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_balanced():\n",
    "   # Create the train dataset from the subjects' posts and label.\n",
    "   txt_path = glob('../data/processed/*')\n",
    "   rows_data = []\n",
    "   max_posts = 25\n",
    "   for path in txt_path:\n",
    "      with open(path, 'r+') as f:\n",
    "         # Split the path, get the filename \n",
    "         # (which contains the subject) and remove '.txt'\n",
    "         subject = path.split('\\\\')[1][:-4]\n",
    "         txt = f.read()\n",
    "         txt = txt.split('\\n')\n",
    "         label = labels[subject]\n",
    "         i = 0\n",
    "         for post in txt:\n",
    "            # For the posts with label 1, store them individually.\n",
    "            if label == 1:\n",
    "               rows_data.append((subject, post, label))\n",
    "            # For the posts with label 0, store only the first 25 posts.\n",
    "            else:\n",
    "               i +=1\n",
    "               if i <= max_posts:\n",
    "                  rows_data.append((subject, post, label))\n",
    "\n",
    "   # Training dataset.\n",
    "   train_data = pd.DataFrame(rows_data, columns=['Subject', 'Text', 'Label'])\n",
    "   return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subject1</td>\n",
       "      <td>Vulcan's ultimate landing at max range is so s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subject1</td>\n",
       "      <td>Is there any defensive item (physical) viable ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subject1</td>\n",
       "      <td>Is it still op? His new passive is a little bi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>subject1</td>\n",
       "      <td>WOOOOOOOOOOOOOOOH (Thoth ult and me running in...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>subject1</td>\n",
       "      <td>That should be Hachiman the new god right?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107824</th>\n",
       "      <td>subject9996</td>\n",
       "      <td>ride ginger cat for the fly ride king bee or t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107825</th>\n",
       "      <td>subject9996</td>\n",
       "      <td>I can add a 1 or 2 aussie eggs</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107826</th>\n",
       "      <td>subject9996</td>\n",
       "      <td>black panter was from the jungle egg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107827</th>\n",
       "      <td>subject9996</td>\n",
       "      <td>feel free to dm me or comment your offer for t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107828</th>\n",
       "      <td>subject9996</td>\n",
       "      <td>Pre-teen is it</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>107829 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Subject                                               Text  Label\n",
       "0          subject1  Vulcan's ultimate landing at max range is so s...      0\n",
       "1          subject1  Is there any defensive item (physical) viable ...      0\n",
       "2          subject1  Is it still op? His new passive is a little bi...      0\n",
       "3          subject1  WOOOOOOOOOOOOOOOH (Thoth ult and me running in...      0\n",
       "4          subject1         That should be Hachiman the new god right?      0\n",
       "...             ...                                                ...    ...\n",
       "107824  subject9996  ride ginger cat for the fly ride king bee or t...      0\n",
       "107825  subject9996                     I can add a 1 or 2 aussie eggs      0\n",
       "107826  subject9996               black panter was from the jungle egg      0\n",
       "107827  subject9996  feel free to dm me or comment your offer for t...      0\n",
       "107828  subject9996                                     Pre-teen is it      0\n",
       "\n",
       "[107829 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_bal = create_dataset_balanced()\n",
    "train_data_bal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    54840\n",
       "0    52989\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_bal[\"Label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subject1</td>\n",
       "      <td>Vulcan's ultimate landing at max range is so s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subject1</td>\n",
       "      <td>Idk if it is broken or not but I've noticed th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subject1</td>\n",
       "      <td>Nope. I've been playing normally. I play EU th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>subject1</td>\n",
       "      <td>How can i rebuild it? Ok thanky you. I'll try ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>subject1</td>\n",
       "      <td>Oh i didn't know that. Let's hope the new syst...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107558</th>\n",
       "      <td>subject9996</td>\n",
       "      <td>Cool! Thx for cheering me up :) I try my best ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107559</th>\n",
       "      <td>subject9996</td>\n",
       "      <td>beautiful   The Last Guest is my idol How can ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107560</th>\n",
       "      <td>subject9996</td>\n",
       "      <td>Oh dm me it's beautiful it's beautiful   unfin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107561</th>\n",
       "      <td>subject9996</td>\n",
       "      <td>Sure Nty can I do an very old doge skateboard ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107562</th>\n",
       "      <td>subject9996</td>\n",
       "      <td>the swan is traded already You mean 2 from my ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>107563 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Subject                                               Text  Label\n",
       "0          subject1  Vulcan's ultimate landing at max range is so s...      0\n",
       "1          subject1  Idk if it is broken or not but I've noticed th...      0\n",
       "2          subject1  Nope. I've been playing normally. I play EU th...      0\n",
       "3          subject1  How can i rebuild it? Ok thanky you. I'll try ...      0\n",
       "4          subject1  Oh i didn't know that. Let's hope the new syst...      0\n",
       "...             ...                                                ...    ...\n",
       "107558  subject9996  Cool! Thx for cheering me up :) I try my best ...      0\n",
       "107559  subject9996  beautiful   The Last Guest is my idol How can ...      0\n",
       "107560  subject9996  Oh dm me it's beautiful it's beautiful   unfin...      0\n",
       "107561  subject9996  Sure Nty can I do an very old doge skateboard ...      0\n",
       "107562  subject9996  the swan is traded already You mean 2 from my ...      0\n",
       "\n",
       "[107563 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_posts = create_dataset_single_posts()\n",
    "train_data_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    54840\n",
       "0    52723\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the distribution of class labels.\n",
    "train_data_posts[\"Label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data_posts[\"Label\"].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing on the new dataset.\n",
    "train_data_bal[\"Text\"] = train_data_bal[\"Text\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_bal[\"Text\"] = train_data_bal[\"Text\"].apply(remove_whitespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_bal[\"Text\"] = train_data_bal[\"Text\"].apply(remove_URLs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_bal[\"Text\"] = train_data_bal[\"Text\"].apply(lambda x: word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_bal[\"Text\"] = train_data_bal[\"Text\"].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_bal[\"Text\"] = train_data_bal[\"Text\"].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_bal[\"Text\"] = train_data_bal[\"Text\"].apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset to a .csv file.\n",
    "train_data_bal.to_csv('../data/train_dataset_bal.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training dataset.\n",
    "train_data_bal = pd.read_csv('../data/train_dataset_bal.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data_bal[\"Text\"]\n",
    "y = train_data_bal[\"Label\"]\n",
    "\n",
    "# Feature extractors.\n",
    "feature_extractors = [\n",
    "CountVectorizer(),\n",
    "TfidfVectorizer(),\n",
    "# Ignore words that appear in less than 20% of posts (rare words).\n",
    "CountVectorizer(min_df=0.2),\n",
    "# Ignore words that appear in more than 80% of posts (frequent words).\n",
    "CountVectorizer(max_df=0.8),\n",
    "CountVectorizer(ngram_range=(2, 2)),\n",
    "CountVectorizer(ngram_range=(3, 3)),\n",
    "TfidfVectorizer(min_df=0.2),\n",
    "TfidfVectorizer(max_df=0.8),\n",
    "TfidfVectorizer(ngram_range=(2, 2)),\n",
    "TfidfVectorizer(ngram_range=(3, 3))\n",
    "]\n",
    "\n",
    "classifiers = [ \n",
    "# LinearSVC(max_iter=100000), \n",
    "# LogisticRegression(max_iter=100000),\n",
    "KNeighborsClassifier(),\n",
    "DecisionTreeClassifier(),\n",
    "RandomForestClassifier()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  3.93  min\n",
      "Experiment:  KNeighborsClassifier() + CountVectorizer()\n",
      "F1 score=  0.6 , 5-fold CV= [0.64331495 0.59233998 0.59258867 0.63897828 0.54439732]\n",
      "Precision=  0.67 , 5-fold CV= [0.70684573 0.69760257 0.7079164  0.71233184 0.52297354]\n",
      "Recall=  0.55 , 5-fold CV= [0.59026258 0.51467907 0.5095733  0.57932166 0.56765135]\n",
      "Accuracy=  0.63 , 5-fold CV= [0.6671149  0.63971066 0.64365204 0.66706853 0.51676327]\n",
      "====================================================================================================\n",
      "Time:  4.18  min\n",
      "Experiment:  KNeighborsClassifier() + TfidfVectorizer()\n",
      "F1 score=  0.32 , 5-fold CV= [0.31965559 0.30685021 0.30235147 0.35004304 0.31258946]\n",
      "Precision=  0.58 , 5-fold CV= [0.60954336 0.37827365 0.63799178 0.63948706 0.61922765]\n",
      "Recall=  0.22 , 5-fold CV= [0.2166302  0.25811451 0.19812181 0.24097374 0.20906273]\n",
      "Accuracy=  0.51 , 5-fold CV= [0.53102105 0.40693685 0.53500881 0.54488547 0.53234408]\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\_School\\Master AI (2020-2022)\\Anul 2\\Semestrul 2\\Bio-medical NLP\\Project - Early Risk Detection of Pathological Gambling\\Github\\CLEF_eRisk\\bioNLP_venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "5 fits failed out of a total of 5.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\_School\\Master AI (2020-2022)\\Anul 2\\Semestrul 2\\Bio-medical NLP\\Project - Early Risk Detection of Pathological Gambling\\Github\\CLEF_eRisk\\bioNLP_venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\_School\\Master AI (2020-2022)\\Anul 2\\Semestrul 2\\Bio-medical NLP\\Project - Early Risk Detection of Pathological Gambling\\Github\\CLEF_eRisk\\bioNLP_venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 390, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"d:\\_School\\Master AI (2020-2022)\\Anul 2\\Semestrul 2\\Bio-medical NLP\\Project - Early Risk Detection of Pathological Gambling\\Github\\CLEF_eRisk\\bioNLP_venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 348, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"d:\\_School\\Master AI (2020-2022)\\Anul 2\\Semestrul 2\\Bio-medical NLP\\Project - Early Risk Detection of Pathological Gambling\\Github\\CLEF_eRisk\\bioNLP_venv\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"d:\\_School\\Master AI (2020-2022)\\Anul 2\\Semestrul 2\\Bio-medical NLP\\Project - Early Risk Detection of Pathological Gambling\\Github\\CLEF_eRisk\\bioNLP_venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 893, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"d:\\_School\\Master AI (2020-2022)\\Anul 2\\Semestrul 2\\Bio-medical NLP\\Project - Early Risk Detection of Pathological Gambling\\Github\\CLEF_eRisk\\bioNLP_venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1347, in fit_transform\n",
      "    X, self.stop_words_ = self._limit_features(\n",
      "  File \"d:\\_School\\Master AI (2020-2022)\\Anul 2\\Semestrul 2\\Bio-medical NLP\\Project - Early Risk Detection of Pathological Gambling\\Github\\CLEF_eRisk\\bioNLP_venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1179, in _limit_features\n",
      "    raise ValueError(\n",
      "ValueError: After pruning, no terms remain. Try a lower min_df or a higher max_df.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  0.16  min\n",
      "Experiment:  KNeighborsClassifier() + CountVectorizer(min_df=0.2)\n",
      "F1 score=  nan , 5-fold CV= [nan nan nan nan nan]\n",
      "Precision=  nan , 5-fold CV= [nan nan nan nan nan]\n",
      "Recall=  nan , 5-fold CV= [nan nan nan nan nan]\n",
      "Accuracy=  nan , 5-fold CV= [nan nan nan nan nan]\n",
      "====================================================================================================\n",
      "Time:  3.91  min\n",
      "Experiment:  KNeighborsClassifier() + CountVectorizer(max_df=0.8)\n",
      "F1 score=  0.6 , 5-fold CV= [0.64331495 0.59233998 0.59258867 0.63897828 0.54439732]\n",
      "Precision=  0.67 , 5-fold CV= [0.70684573 0.69760257 0.7079164  0.71233184 0.52297354]\n",
      "Recall=  0.55 , 5-fold CV= [0.59026258 0.51467907 0.5095733  0.57932166 0.56765135]\n",
      "Accuracy=  0.63 , 5-fold CV= [0.6671149  0.63971066 0.64365204 0.66706853 0.51676327]\n",
      "====================================================================================================\n",
      "Time:  3.45  min\n",
      "Experiment:  KNeighborsClassifier() + CountVectorizer(ngram_range=(2, 2))\n",
      "F1 score=  0.41 , 5-fold CV= [0.60527598 0.16967126 0.49405167 0.43760005 0.34922133]\n",
      "Precision=  0.63 , 5-fold CV= [0.48947428 0.80557707 0.46260344 0.73499677 0.67011643]\n",
      "Recall=  0.39 , 5-fold CV= [0.79285193 0.0948213  0.53008753 0.31154267 0.2361415 ]\n",
      "Accuracy=  0.52 , 5-fold CV= [0.47407957 0.52800705 0.44783455 0.59273857 0.55237654]\n",
      "====================================================================================================\n",
      "Time:  68.35  min\n",
      "Experiment:  KNeighborsClassifier() + CountVectorizer(ngram_range=(3, 3))\n",
      "F1 score=  0.04 , 5-fold CV= [0.02234436 0.01627486 0.04392673 0.03298673 0.09208511]\n",
      "Precision=  0.9 , 5-fold CV= [0.94656489 0.97826087 0.88848921 0.9787234  0.69181586]\n",
      "Recall=  0.02 , 5-fold CV= [0.01130562 0.00820569 0.02252006 0.01677608 0.04932531]\n",
      "Accuracy=  0.5 , 5-fold CV= [0.49684689 0.49550218 0.50143745 0.49976815 0.50530953]\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\_School\\Master AI (2020-2022)\\Anul 2\\Semestrul 2\\Bio-medical NLP\\Project - Early Risk Detection of Pathological Gambling\\Github\\CLEF_eRisk\\bioNLP_venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "5 fits failed out of a total of 5.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\_School\\Master AI (2020-2022)\\Anul 2\\Semestrul 2\\Bio-medical NLP\\Project - Early Risk Detection of Pathological Gambling\\Github\\CLEF_eRisk\\bioNLP_venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\_School\\Master AI (2020-2022)\\Anul 2\\Semestrul 2\\Bio-medical NLP\\Project - Early Risk Detection of Pathological Gambling\\Github\\CLEF_eRisk\\bioNLP_venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 390, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"d:\\_School\\Master AI (2020-2022)\\Anul 2\\Semestrul 2\\Bio-medical NLP\\Project - Early Risk Detection of Pathological Gambling\\Github\\CLEF_eRisk\\bioNLP_venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 348, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"d:\\_School\\Master AI (2020-2022)\\Anul 2\\Semestrul 2\\Bio-medical NLP\\Project - Early Risk Detection of Pathological Gambling\\Github\\CLEF_eRisk\\bioNLP_venv\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"d:\\_School\\Master AI (2020-2022)\\Anul 2\\Semestrul 2\\Bio-medical NLP\\Project - Early Risk Detection of Pathological Gambling\\Github\\CLEF_eRisk\\bioNLP_venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 893, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"d:\\_School\\Master AI (2020-2022)\\Anul 2\\Semestrul 2\\Bio-medical NLP\\Project - Early Risk Detection of Pathological Gambling\\Github\\CLEF_eRisk\\bioNLP_venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 2077, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"d:\\_School\\Master AI (2020-2022)\\Anul 2\\Semestrul 2\\Bio-medical NLP\\Project - Early Risk Detection of Pathological Gambling\\Github\\CLEF_eRisk\\bioNLP_venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1347, in fit_transform\n",
      "    X, self.stop_words_ = self._limit_features(\n",
      "  File \"d:\\_School\\Master AI (2020-2022)\\Anul 2\\Semestrul 2\\Bio-medical NLP\\Project - Early Risk Detection of Pathological Gambling\\Github\\CLEF_eRisk\\bioNLP_venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1179, in _limit_features\n",
      "    raise ValueError(\n",
      "ValueError: After pruning, no terms remain. Try a lower min_df or a higher max_df.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  0.15  min\n",
      "Experiment:  KNeighborsClassifier() + TfidfVectorizer(min_df=0.2)\n",
      "F1 score=  nan , 5-fold CV= [nan nan nan nan nan]\n",
      "Precision=  nan , 5-fold CV= [nan nan nan nan nan]\n",
      "Recall=  nan , 5-fold CV= [nan nan nan nan nan]\n",
      "Accuracy=  nan , 5-fold CV= [nan nan nan nan nan]\n",
      "====================================================================================================\n",
      "Time:  4.14  min\n",
      "Experiment:  KNeighborsClassifier() + TfidfVectorizer(max_df=0.8)\n",
      "F1 score=  0.32 , 5-fold CV= [0.31965559 0.30685021 0.30235147 0.35004304 0.31258946]\n",
      "Precision=  0.58 , 5-fold CV= [0.60954336 0.37827365 0.63799178 0.63948706 0.61922765]\n",
      "Recall=  0.22 , 5-fold CV= [0.2166302  0.25811451 0.19812181 0.24097374 0.20906273]\n",
      "Accuracy=  0.51 , 5-fold CV= [0.53102105 0.40693685 0.53500881 0.54488547 0.53234408]\n",
      "====================================================================================================\n",
      "Time:  3.76  min\n",
      "Experiment:  KNeighborsClassifier() + TfidfVectorizer(ngram_range=(2, 2))\n",
      "F1 score=  0.25 , 5-fold CV= [0.26062039 0.22039026 0.38347536 0.19716555 0.18268183]\n",
      "Precision=  0.63 , 5-fold CV= [0.60815781 0.64640639 0.59414466 0.6755603  0.62593383]\n",
      "Recall=  0.16 , 5-fold CV= [0.1658461  0.13284099 0.28309628 0.1154267  0.10694748]\n",
      "Accuracy=  0.52 , 5-fold CV= [0.52142261 0.52202541 0.53704906 0.52193267 0.51328542]\n",
      "====================================================================================================\n",
      "Time:  3.28  min\n",
      "Experiment:  KNeighborsClassifier() + TfidfVectorizer(ngram_range=(3, 3))\n",
      "F1 score=  0.24 , 5-fold CV= [0.0063567  0.00363967 0.5440991  0.01070004 0.64252517]\n",
      "Precision=  0.73 , 5-fold CV= [0.79545455 0.90909091 0.45860948 0.98333333 0.49730216]\n",
      "Recall=  0.32 , 5-fold CV= [0.0031911  0.00182349 0.66876368 0.00537929 0.90754923]\n",
      "Accuracy=  0.48 , 5-fold CV= [0.49262728 0.49225633 0.43002875 0.4941111  0.48638998]\n",
      "====================================================================================================\n",
      "Time:  6.4  min\n",
      "Experiment:  DecisionTreeClassifier() + CountVectorizer()\n",
      "F1 score=  0.67 , 5-fold CV= [0.68291768 0.65036722 0.66225677 0.70212865 0.63502277]\n",
      "Precision=  0.7 , 5-fold CV= [0.70281745 0.69181743 0.70388997 0.72105314 0.69456475]\n",
      "Recall=  0.63 , 5-fold CV= [0.66411379 0.61360321 0.62527352 0.68417214 0.5848833 ]\n",
      "Accuracy=  0.68 , 5-fold CV= [0.68635816 0.66447185 0.67564685 0.70476676 0.65805704]\n",
      "====================================================================================================\n",
      "Time:  6.69  min\n",
      "Experiment:  DecisionTreeClassifier() + TfidfVectorizer()\n",
      "F1 score=  0.68 , 5-fold CV= [0.70068215 0.65879404 0.67008206 0.71284378 0.64759959]\n",
      "Precision=  0.7 , 5-fold CV= [0.70851976 0.69884446 0.69840799 0.72417686 0.69408821]\n",
      "Recall=  0.65 , 5-fold CV= [0.69301605 0.62308534 0.64396426 0.70185996 0.60694748]\n",
      "Accuracy=  0.68 , 5-fold CV= [0.69887786 0.67175183 0.67750162 0.71241769 0.66403895]\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\_School\\Master AI (2020-2022)\\Anul 2\\Semestrul 2\\Bio-medical NLP\\Project - Early Risk Detection of Pathological Gambling\\Github\\CLEF_eRisk\\bioNLP_venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "5 fits failed out of a total of 5.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\_School\\Master AI (2020-2022)\\Anul 2\\Semestrul 2\\Bio-medical NLP\\Project - Early Risk Detection of Pathological Gambling\\Github\\CLEF_eRisk\\bioNLP_venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\_School\\Master AI (2020-2022)\\Anul 2\\Semestrul 2\\Bio-medical NLP\\Project - Early Risk Detection of Pathological Gambling\\Github\\CLEF_eRisk\\bioNLP_venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 390, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"d:\\_School\\Master AI (2020-2022)\\Anul 2\\Semestrul 2\\Bio-medical NLP\\Project - Early Risk Detection of Pathological Gambling\\Github\\CLEF_eRisk\\bioNLP_venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 348, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"d:\\_School\\Master AI (2020-2022)\\Anul 2\\Semestrul 2\\Bio-medical NLP\\Project - Early Risk Detection of Pathological Gambling\\Github\\CLEF_eRisk\\bioNLP_venv\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"d:\\_School\\Master AI (2020-2022)\\Anul 2\\Semestrul 2\\Bio-medical NLP\\Project - Early Risk Detection of Pathological Gambling\\Github\\CLEF_eRisk\\bioNLP_venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 893, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"d:\\_School\\Master AI (2020-2022)\\Anul 2\\Semestrul 2\\Bio-medical NLP\\Project - Early Risk Detection of Pathological Gambling\\Github\\CLEF_eRisk\\bioNLP_venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1347, in fit_transform\n",
      "    X, self.stop_words_ = self._limit_features(\n",
      "  File \"d:\\_School\\Master AI (2020-2022)\\Anul 2\\Semestrul 2\\Bio-medical NLP\\Project - Early Risk Detection of Pathological Gambling\\Github\\CLEF_eRisk\\bioNLP_venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1179, in _limit_features\n",
      "    raise ValueError(\n",
      "ValueError: After pruning, no terms remain. Try a lower min_df or a higher max_df.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  0.15  min\n",
      "Experiment:  DecisionTreeClassifier() + CountVectorizer(min_df=0.2)\n",
      "F1 score=  nan , 5-fold CV= [nan nan nan nan nan]\n",
      "Precision=  nan , 5-fold CV= [nan nan nan nan nan]\n",
      "Recall=  nan , 5-fold CV= [nan nan nan nan nan]\n",
      "Accuracy=  nan , 5-fold CV= [nan nan nan nan nan]\n",
      "====================================================================================================\n",
      "Time:  6.41  min\n",
      "Experiment:  DecisionTreeClassifier() + CountVectorizer(max_df=0.8)\n",
      "F1 score=  0.67 , 5-fold CV= [0.6869056  0.64983831 0.66212495 0.70353446 0.63444318]\n",
      "Precision=  0.7 , 5-fold CV= [0.70646622 0.69039073 0.70428615 0.71998473 0.69356409]\n",
      "Recall=  0.64 , 5-fold CV= [0.66839898 0.61378556 0.62472648 0.68781911 0.58460977]\n",
      "Accuracy=  0.68 , 5-fold CV= [0.69011407 0.66359084 0.67573959 0.70518409 0.65736147]\n",
      "====================================================================================================\n",
      "Time:  281.45  min\n",
      "Experiment:  DecisionTreeClassifier() + CountVectorizer(ngram_range=(2, 2))\n",
      "F1 score=  0.58 , 5-fold CV= [0.62879235 0.55631555 0.56137368 0.61087637 0.52888139]\n",
      "Precision=  0.75 , 5-fold CV= [0.76485957 0.74209246 0.74168913 0.76719504 0.73051583]\n",
      "Recall=  0.47 , 5-fold CV= [0.53382567 0.44493071 0.45158643 0.50747629 0.41447848]\n",
      "Accuracy=  0.65 , 5-fold CV= [0.67944913 0.63906149 0.64110173 0.6711954  0.62443775]\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "start_ML_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 4 ML models\n",
    "Make predictions using the top 4 ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c2df39527199e9c3424817245acb9a622fe910bfbc420b32efee2c316061d83c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('bioNLP_venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
