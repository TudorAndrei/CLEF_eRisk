{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as xml\n",
    "from glob import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import html\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path = r\"../data/raw/*\"\n",
    "path = glob(raw_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "    # Remove unhandled unicode chars (#1231)\n",
    "    text = re.sub(r\"( #\\d*;)\",\n",
    "              lambda m: html.unescape('&' +m.group(1)[1:]),\n",
    "              text.rstrip())\n",
    "\n",
    "    # Replace '&amp;' with '&'\n",
    "    text = re.sub(r'&amp;', '&', text)\n",
    "\n",
    "    # Remove trailing whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Combine user posts in single `.txt` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "processed_data_path = '../data/processed'\n",
    "if not os.path.isdir(processed_data_path):\n",
    "    os.mkdir(processed_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2348/2348 [00:00<00:00, 6173.94it/s]\n"
     ]
    }
   ],
   "source": [
    "for p in tqdm(path):\n",
    "    subject = p.split('/')[-1][:-4] + \".txt\"\n",
    "    # print(subject)\n",
    "    subject_path = os.path.join(processed_data_path, subject)\n",
    "    if os.path.isfile(subject_path):\n",
    "        continue\n",
    "    root = xml.parse(p).getroot()\n",
    "    for text in root.findall(\"WRITING/TEXT\"):\n",
    "        with open(subject_path, 'a') as f:\n",
    "            if text.text is not None:\n",
    "                f.write(text.text)\n",
    "            else:\n",
    "                f.write(\" \")\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_labels = '../data/risk_golden_truth.txt'\n",
    "# labels = {}\n",
    "# with open(path_labels, 'r') as f:\n",
    "#     for line in f:\n",
    "#         subject, label = line.split()\n",
    "#         labels[subject] = int(label)\n",
    "# labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Each user's post becomes a user_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data_path = '../data/split'\n",
    "if not os.path.isdir(split_data_path):\n",
    "    os.mkdir(split_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_labels = os.path.join('../data/', \"risk_golden_truth.txt\")\n",
    "users = []\n",
    "with open(path_labels, 'r') as f:\n",
    "    for line in f:\n",
    "        subject, label = line.split()\n",
    "        users.append((subject, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2348/2348 [01:42<00:00, 22.88it/s]\n"
     ]
    }
   ],
   "source": [
    "ground_truth_subtexts = os.path.join('../data/', 'risk_golden_truth_split.txt')\n",
    "for subject, label in tqdm(users):\n",
    "    user_file = os.path.join('../data', 'raw', subject+'.xml')\n",
    "\n",
    "    root = xml.parse(user_file).getroot()\n",
    "    for i, text in enumerate(root.findall(\"WRITING/TEXT\")):\n",
    "        subject_subtext_path = os.path.join(split_data_path, f\"{subject}_{i}.txt\")\n",
    "        # xml to txt\n",
    "        if text.text is None:\n",
    "            continue\n",
    "        with open(subject_subtext_path, 'w') as f:\n",
    "            f.write(text.text)\n",
    "        # generate labels\n",
    "        with open(ground_truth_subtexts, 'a') as f:\n",
    "            f.write(f'{subject}_{i} {label}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: Text chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_data_path = '../data/chunked'\n",
    "if not os.path.isdir(chunk_data_path):\n",
    "    os.mkdir(chunk_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_labels = os.path.join('../data/', \"risk_golden_truth.txt\")\n",
    "users = []\n",
    "with open(path_labels, 'r') as f:\n",
    "    for line in f:\n",
    "        subject, label = line.split()\n",
    "        users.append((subject, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 164\n",
      "0: 2184\n"
     ]
    }
   ],
   "source": [
    "ones = sum([int(x) for y, x in users])\n",
    "print(\"1:\", ones)\n",
    "print(\"0:\", len(users) - ones)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2348 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "ground_truth_chunks = os.path.join('../data/', 'risk_golden_truth_chunks.txt')\n",
    "for subject, label in tqdm(users):\n",
    "    if label == '1':\n",
    "        CHUNK_DELAY = 3\n",
    "    if label == '0':\n",
    "        CHUNK_DELAY = 5\n",
    "    # print(subject)\n",
    "    # subject = 'subject4931'\n",
    "    user_file = os.path.join('../data', 'raw', subject + '.xml')\n",
    "\n",
    "    root = xml.parse(user_file).getroot()\n",
    "    sample_length = 0\n",
    "    texts = []\n",
    "    current = \"\"\n",
    "    for i, text in enumerate(root.findall(\"WRITING/TEXT\")):\n",
    "        if text.text is None:\n",
    "            continue\n",
    "        p_text = text_preprocessing(text.text)\n",
    "        texts.append((p_text, len(p_text.split())))\n",
    "\n",
    "    chunks = []\n",
    "\n",
    "    while texts:\n",
    "        # print(len(texts))\n",
    "        text_iter = iter(texts)\n",
    "        current_text = next(text_iter)\n",
    "        sub_chunk = ''\n",
    "        passed = 0\n",
    "\n",
    "        while True:\n",
    "            current_subchunk_size = len(sub_chunk.split())\n",
    "            # print(current_subchunk_size)\n",
    "            # print(current_text[1])\n",
    "            if current_subchunk_size + current_text[1] > MAX_LEN:\n",
    "                chunks.append(sub_chunk)\n",
    "                break\n",
    "            else:\n",
    "                sub_chunk += \" \" + current_text[0]\n",
    "                passed += 1\n",
    "            try:\n",
    "                current_text = next(text_iter)\n",
    "            except StopIteration:\n",
    "                chunks.append(sub_chunk)\n",
    "                break\n",
    "    \n",
    "        texts = texts[min(passed, CHUNK_DELAY):]\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        subject_subtext_path = os.path.join(chunk_data_path, f\"{subject}_{i}.txt\")\n",
    "        with open(subject_subtext_path, 'w') as f:\n",
    "            f.write(chunk)\n",
    "        # # generate labels\n",
    "        with open(ground_truth_chunks, 'a') as f:\n",
    "            f.write(f'{subject}_{i} {label}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_chunks = os.path.join('../data/', \"risk_golden_truth_chunks.txt\")\n",
    "chunk_users = []\n",
    "with open(ground_truth_chunks, 'r') as f:\n",
    "    for line in f:\n",
    "        subject, label = line.split()\n",
    "        chunk_users.append((subject, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 17471\n",
      "0: 159766\n"
     ]
    }
   ],
   "source": [
    "ones = sum([int(x) for y, x in chunk_users])\n",
    "print(\"1:\", ones)\n",
    "print(\"0:\", len(chunk_users) - ones)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bc7067ad0587ee2533d0568fff1e23349b43564da49d2e99835e9871e4935780"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
